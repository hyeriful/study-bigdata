# Hadoop
**대량의 자료를 처리할 수 있는 큰 컴퓨터 클러스터에서 동작하는 분산 응용 프로그램을 지원하는 프레웨어 자바 소프트웨어 프레임워크**

- RDBMS와 맵리듀스 비교

||전통적인 RDBMS|맵리듀스|
|------|---|---|
|데이터 크기|기가바이트|페타바이트|
|접근 방식|대화형과 일괄 처리 방식|일괄 처리 방식|
|변경|여러 번 읽고 쓰기|한 번 쓰고 여러 번 읽기|
|구조|쓰기 기준 스키마|일기 기준 스키마|
|무결성|높음|낮음|
|확장성|비선형|선형|

RDBMS는 수 밀리초 ~ 수 초 정도의 짧은 응답시간을 요구하고, Hadoop에서는 분산처리를 위한 전처리가 필요하여 최저 10 ~ 20초 정도의 오버헤드가 발생한다.  
맵리듀스는 비정형 분석과 같이 일괄 처리 방식으로 전체 데이터셋을 분석할 필요가 있는 문제에 적합하다. RDBMS는 상대적으로 작은 양의 데이터를 낮은 지연 시간에 추출하고 변경하기 위해 데이터셋을 색인하기 때문에 특정 쿼리와 데이터 변경에 적합하다. 맵리듀스는 데이터를 한 번 쓰고 여러 번 읽는 애플리케이션에 적합하지만, 관계형 데이터베이스는 지속적으로 변경되는 데이터셋에 적합하다고 할 수 있다.  

- hadoop cluster  
클러스터란, 특정한 기능수행을 위해 여러 대의 컴퓨터가 네트워크로 연결된 것을 의미하며, 이때 클러스터를 구성하는 개별 컴퓨터를 node라고 한다.
여러 개의 노드가 모여 하나의 rack을 구성하고 rack이 모여 하나의 hadoop cluster를 구축하게 된다.
rack은 물리적으로 같은 network의 switch에 모두 연결 되어 있다. 그렇기 때문에 두 노드의 대역폭은 다른 rack에 있는 노드보다 크게 된다. 즉 데이터의 이동을 할 수 있는 폭이 크기 때문에 데이터 속도가 빠르다. network의 다른 switch에 연결되어 있는 rack으로 인해 성능저하가 발생할 수 있다.
<img width="472" alt="hadoop cluster" src="https://user-images.githubusercontent.com/55703132/110307981-e26e2c00-8042-11eb-9134-5f345a3caa68.png">

## 1. 맵리듀스
대용량의 데이터 처리를 위한 분산 병렬 프로그래밍 모델, 소프트웨어 프레임워크  
맵 리듀스 프레임워크를 이용하면 대규모 분산 컴퓨팅 환경에서 대량의 데이터를 병렬로 분석 가능  
프로그래머가 직접 작성하는 맵과 리듀스라는 두개의 메서드로 구성  

- Map  
흩어져 있는 데이터를 연관성 있는 데이터들로 분류하는 작업. (key, value 형태)
- Reduce  
Map에서 출력된 데이터에서 중복 데이터를 제거하고 원하는 데이터를 추출하는 작업

하둡은 데이터의 일부분이 저장된 클러스터의 각 머신에서 맵리듀스 프로그램을 실행한다. 이를 위해 하둡은 YARN(하둡 자원 관리 시스템)을 이용한다.

### 데이터 흐름
**잡Job**은 클라이언트가 수행하는 작업의 기본 단위이다. 하둡은 잡을 **map task**와 **reduce task**로 나누어 실행한다. 각 태스크는 YARN을 이용하여 스케줄링되고 클러스터의 여러 노드에서 실행된다. 특정 노드의 태스크가 하나 실패하면 자동으로 다른 노드를 재할당 하여 다시 실행된다.  

하둡은 맵리듀스 잡의 입력을 input split(또는 단순히 split)이라고 부르는 고정 크기 조각으로 분리한다. 하둡은 각 스플릿마다 하나의 맵 태스를 생성하고 스플릿의 각 레코드를 사용자 정의 맵 함수로 처리한다. 일반적인 잡의 적절한 스플릿 크기는 HDFS 블록의 기본 크기인 128mb가 적당하다고 알려져 있다.  

하둡은 HDFS 내의 입력 데이터가 있는 노드에서 맵 태스크를 실행할 때 가장 빠르게 작동한다(a). 이를 **data locality optimization(데이터 지역성 최적화)** 라고 하는데, 클러스터의 중요한 공유자원인 네트워크 대역폭을 사용하지 않는 방법이다. 그러나 맵 태스크의 입력 스플릿에 해당하는 HDFS 블록 복제본이 저장된 세 개의 노드 모두 다른 맵 태스크를 실행하여 여유가 없는 상황(데이터 지역성을 위한 가용 슬롯이 없는)이 발생할 수도 있다. 이런 상황에서 잡 스케줄러는 블록 복제본이 저장된 동일 랙에 속한 다른 노드에서 가용한 맵 슬롯을 찾는다(b). 또한 아주 드문 일이지만 데이터 복제본이 저장된 노드가 없는 외부 랙의 노드가 선택될 수도 있는데, 이때에는 랙 간 네트워크 전송이 불가피하게 일어난다(c).

<img width="350" alt="맵 태스크" src="https://user-images.githubusercontent.com/55703132/110323875-90d09c00-8058-11eb-8c5f-2a25bdaa6fad.png">

맵 태스크 결과는 HDFS가 아닌 로컬 디스크에 저장된다. 리듀스의 결과는 HDFS애 저장된다.
리듀스 태스크로 모든 결과를 보내기 전에 맵 태스크가 실패한다면 하둡은 자동으로 해당 맵 태스크를 다른 노드에 할당하여 맵의 출력을 다시 생성할 것이다. 리듀스 태스크는 일반적으로 모든 매퍼의 출력 결과를 입력으로 받기 때문에 데이터 지역성의 장점이 없다. 정렬된 맵의 모든 결과는 네트워크를 통해 일단 리듀스 태스크가 실행 중인 노드로 전송되고, 맵의 모든 결과를 병합 후 사용자 정의 리듀스 함수로 전달된다.  
리듀스 출력에 대한 HDFS 블록의 첫번째 복제본은 로컬 노드에 저장되고, 나머지 복제본은 외부 랙에 저장된다.

## 2. HDFS (Hadoop Distributed FileSystem)
분산 파일 시스템이란?  
네트워크로 연결된 여러 머신의 스토리지를 관리하는 파일시스템

하둡은 HDFS라는 분산 파일시스템을 제공한다.

### 블록  
> HDFS 블록이 큰 이유는?  
> HDFS 블록(128 MB)은 디스크 블록(512 Btye)에 비해 상당히 크다. 그 이유는 **탐색 비용을 최소화**하기 위해서다. 블록이 매우 크면 블록의 시작점을 탐색하는 데 걸리는 시간을 줄일 수 있고 데이터를 전송하는 데 더 많은 시간을 할애할 수 있다.

분산 파일시스템에 **블록 추상화**의 개념이 도입하면서 얻게 된 이득이 있다.
1. 파일 하나의 크기가 단일 디스크의 용량보다 더 커질 수 있다.  
하나의 파일을 구성하는 여러 개의 블록이 동일한 디스크에만 저장될 필요가 없으므로 클러스터에 있는 어떤 디스크에도 저장될 수 있다. (-> 파일은 블록으로 나뉠 수 있고, 블록은 디스크에 저장된다)

2. 파일 단위보다는 블록단위로 추상화를 하면 스토리지의 서브시스템을 단순하게 만들 수 있다.  
블록은 고정 크기고 저장에 필요한 디스크 용량만 계산하면 되기 때문에 메타데이터에 대한 고민을 덜 수 있다. 블록은 단지 저장된 데이터의 청크일 뿐이고 권한 정보와 같은 파일의 메타데이터는 블록과 함께 저장될 필요가 없으므로 별도의 시스템에서 다루도록 분리할 수 있다.

3. 블록은 내고장성(fault tolerance)과 가용성(availability)을 제공하는 데 필요한 복제(replication)를 구현할 때 매우 적합하다.  
블록의 손상과 디스크 및 머신의 장애에 대처하기 위해 각 블록은 물리적으로 분리된 다수의 머신(보통 3개)에 복제된다.

### 네임노드와 데이터노드
HDFS 클러스터는 master-worker 패턴으로 동작하는 두 종류의 노드(**마스터인 하나의 네임노드와 워커인 여러 개의 데이터노드**)로 구성되어 있다.  
**네임노드는 파일 시스템의 네임스페이스를 관리**한다. 네임노드는 파일시스템 트리와 그 트리에 포함된 모든 파일과 디렉터리에 대한 메타데이터를 유지한다. 이 정보는 namespace image와 edit log라는 두 종류의 파일로 로컬 디스크에 영속적으로 저장된다. 또한 **파일에 속한 모든 블록이 어느 데이터노드에 있는지 파악하고 있다.** 하지만 블록의 위치 정보는 시스템이 시작할 때 모든 데이터노드로부터 받아서 재구성하기 때문에 디스크에 영속적으로 저장하지는 않는다.  

HDFS 클라이언트는 사용자를 대신해서 네임노드와 데이터노드 사이에서 통신하고 파일시스템에 접근.  

**데이터노드는 클라이언트나 네임노드의 요청이 있을 때 블록을 저장하고 탐색하며, 저장하고 있는 블록의 목록을 주기적으로 네임노드에 보고한다.** 데이터 노드는 디스크에 저장된 블록을 읽는다.

네임노드가 없으면 파일시스템은 동작하지 않는다. 따라서 네임노드의 장애복구 기능은 필수적!  
하둡은 두가지 메커니즘 제공  
1. 파일로 백업  
네임노드가 다수의 파일시스템에 영구적인 상태를 저장하도록. 주로 권장하는 방법은 로컬 디스크와 원격의 NFS(Network FileSystem) 마운트 두 곳에 동시에 백업하는 것이다.  
2. 보조 네임노드(secondary namenode) 운영  
secondary namenode의 주 역할은 edit log가 너무 커지지 않도록 주기적으로 namespace image를 edit log와 병합하여 새로운 namespace image를 만드는 것이다. 또한 secondary namenode는 주 네임노드에 장애가 발생할 것을 대비해서 namespace image 복제본을 보관하는 역할도 맡는다.  

### HDFS federation
네임노드는 파일시스템의 모든 파일과 각 블록에 대한 참조 정보를 메모리에서 관리한다. 따라서 파일이 매우 많은 대형 클러스에서 **확장성**에 가장 큰 걸림돌이 되는 것은 바로 **메모리**다. 네임노드의 확장성 문제를 해결하기 위해 하둡은 **HDFS federation(연합체)** 을 지원하고 있다. HDFS federation을 활용하면 각각의 네임노드가 파일시스템의 네임스페이스(소속을 나타낸다?) 일부를 나누어 관리하는 방식으로 새로운 네임노드를 추가할 수 있다. (ex. namenode A는 /user 디렉터리 아래 모든 파일관리. namenode B는 /share 디렉터리 아래 모든 파일관리)  

HDFS federation을 적용하면 각 네임노드는 네임스페이스의 메타데이터를 구성하는 **namespace volume**과 네임스페이스에 포함된 파일의 전체 블록을 보관하는 **block pool**을 관리한다. namespace volume은 서로 독립되어 있으며, 따라서 네임노드는 서로 통신할 필요가 없고, 특정 네임노드에 장애가 발생해도 다른 네임노드가 관리하는 네임스페이스의 가용성에 영향을 주지 않는다. 하지만 block pool의 저장소는 분리되어 있지 않다! 모든 데이터노드는 클러스터의 각 네임노드마다 등록되어 있고, 여러 block poll로부터 블록을 저장한다.

### HDFS 고가용성(HA: high availability)
※ HA: 서버와 네트워크, 프로그램 등의 정보 시스템이 상당히 오랜 기간 동안 지속적으로 정상 운영이 가능한 성질  
※ 네임노드는 single point of failure(SPOF): 네임노드에 장애가 발생하면 맵리듀스 잡을 포함하여 모든 클라이언트가 파일을 읽거나 쓰거나 조회할 수 없게 된다.

High avaliability은 active-standby 상태로 설정된 한 쌍의 네임노드로 구현된다. active namenode에 장애가 발생하면 standby namenode가 그 역할을 이어받아 큰 중단 없이 클라이언트의 요청을 처리한다.  
- 네임노드는 edit log를 공유하기 위해 HA shared storage를 반드시 사용해야 한다. standby namenode가 활성화되면 먼저 기손 active namenode의 상태를 동기화하기 위해 공유 edit log를 읽고, 이어서 active namenode에 새로 추가된 항목도 마저 읽는다.
- 데이터노드는 block report를 두 개의 네임노드에 보내야 한다. 블랙 매핑 정보는 디스크가 아닌 네임노드의 메모리에 보관되기 때문.
- HA에서 standby namenode는 secondary namenode의 역할을 포함하고 있으며, active namenode namespace의 체크포인트 작업을 주기적으로 수행한다.

### 데이터 흐름
- HDFS로부터 데이터 **읽기**  

<img width="450" alt="hdfs-data-flow-read" src="https://user-images.githubusercontent.com/55703132/111037294-c4e7fa80-8466-11eb-8ef6-7cafe8d05bd7.JPG">

1. 클라이언트가 원하는 파일을 연다.  

2. DistributedFileSystem은 파일의 첫 번째 블록 위치를 파악하기 위해 RPC을 사용하여 네임노드를 호출한다.  
네임노드는 블록별로 해당 블록의 복제본을 가진 데이터노드의 주소를 반환. 이때 클러스터의 네트워크 위상에 따라 클라이언트와 가까운 순으로 데이터노드가 정렬 (대역폭: 동일 노드 > 동일 랙의 다른 노드 > 동일 데이터 센터에 있는 다른 랙의 노드 > 다른 데이터 센터에 있는 노드)
<details>
<summary>RPC (remote procedure call)</summary>
<div markdown="1">

RPC(Remote Procedure call)이란, 별도의 원격 제어를 위한 코딩 없이 다른 주소 공간에서 리모트의 함수나 프로시저를 실행 할 수 있게 해주는 프로세스간 통신.  
즉, 위치에 상관없이 RPC를 통해 개발자는 위치에 상관없이 원하는 함수를 사용할 수 있다.

운영체제를 공부하다 보며 프로세스간 통신을 위해 IPC(inter-Process Communication)을 이용하는 내용을 볼 수 있는데, RPC는 IPC 방법의 한 종류로 원격지의 프로세스에 접근하여 프로시저 또는 함수를 호출하여 사용하는 방법을 말한다.

</div>
</details>

3. DFS가 FSDataInputStream을 반환. 클라이언트는 스트림을 읽기 위해 read() 메서드 호출한다.

4. DFSInputStream은 가장 가까운(첫번째) 데이터노드와 연결한다. 해당 스트림에 대해 read() 메서드를 반복적으로 호출하면 데이터노드에서 클라이언트로 모든 데이터가 전송된다.

5. 블록의 끝에 도달하면 DFSInputStream은 데이터노드의 연결을 닫고 다음 블록의 데이터노드를 찾는다.
클라이언트는 스트림을 통해 블록을 순서대로 하나씩 읽는다. 클라이언트는 다음 블록의 데이터노드 위치를 얻기 위해 네임노드를 호출한다.

6. 모든 블록에 대한 읽기가 끝나면 클라이언트는 close() 메서드를 호출한다.

이러한 설계의 핵심은 **클라이언트는 데이터를 얻기 위해 데이터노드에 직접적으로 접촉하고, 네임노드는 각 블록에 적합한 데이터노드를 안내해준다는 것!**
데이터 트래픽은 클러스터에 있는 모든 데이터노드에 고르게 분산되므로 HDFS는 동시에 실행되는 클라이언트의 수를 크게 늘릴 수 있다.

<br>

- 데이터를 HDFS에 **쓰기**

<img width="450" alt="hdfs-data-flow-write" src="https://user-images.githubusercontent.com/55703132/111037865-58222f80-8469-11eb-8c89-6476f1f78504.JPG">

1. create()를 호출하여 파일을 생성한다.

2. DFS은 파일시스템의 네임스페이스에 새로운 파일을 생성하기 위해 네임노드에 RPC 요청을 보낸다.
네임노드는 요청한 파일과 동일한 파일이 이미 존재하는지, 클라리언트가 파일을 생성할 권한을 가지고 있는지 등 다양한 검사를 수행. 검사 통과하면 네임노드는 새로운 파일의 레코드를 만듦.

3. DFS는 데이터를 쓸 수 있도록 FSDataOutputStream을 반환. 클라이언트가 데이터를 쓴다.
(읽을 때와 마찬가지로 FSDataOutputStream은 데이터노드와 네임노드의 통신을 처리하는 DFSOutputStream으로 래핑된다.)

4.
   1. DFSOutputStream은 데이터를 데이터를 패킷으로 분리하고, **data queue**라 불리는 내부 큐로 패킷을 보낸다.
   2. 네임노드에 복제본을 저장할 데이터노드의 목록을 요청한다.
   3. 데이터노드 목록에 포함된 노드는 파이프라인을 형성. 복제 수준이 3이면 세 개의 노드가 파이프라인에 속하게 된다.
   4. DataStreamer(데이터 큐에 있는 패킷을 처리)는 파이프라인의 첫 번째 데이터노드로 패킷을 전송 -> 첫 번째 데이터노드는 각 패킷을 저장하고 그것을 파이프라인의 두 번째 데이터노드로 보냄 -> 두번째 데이토노드는 받은 패킷을 저장하고 마지막 데이터노드로 전달

5. DFSOutputStream은 데이터노드의 승인 여부를 기다리는 **ack queue**라 불리는 내부 패킷 큐를 유지한다. ack queue에 있는 패킷은 파이프라인의 모든 데이터노드로부터 ack 응답을 받아야 제거된다.

6. 데이터 쓰기를 ㅗ안료할 때 클라이언트는 close() 메서드를 호출한다.

7. 데이터노드 파이프라인에 남아 있는 모든 패킷을 flush하고 승인이 나기를 기다린다. 모든 패킷이 완전히 전송되면 네임노드에 'file is complete' 신호를 보낸다.

<br>

복제본 배치

<img width="300" alt="replication_pipeline" src="https://user-images.githubusercontent.com/55703132/111065116-6a4cae00-84fb-11eb-89be-97c06b13f6ee.jpg">

첫 번째 복제본을 클라이언트와 같은 노드에 배치한다 (만약 클라이언트가 클러스터 외부에 있으면(데이터노드가 아니면) 무작위로 노드 선택). 두 번째 복제본은 첫 번째 노드와 다른 랙의 노드에 배치된다. 세 번째 복제본은 두 번째 노드와 같은 랙의 다른 노드에 배치된다. 그 이상의 복제본은 클러스터에서 무작위로 선택하여 배치한다.  


## 3. YARN
클러스터 자원 관리 시스템. (Yet Another Resource Negotiator)
