# Hadoop
**대량의 자료를 처리할 수 있는 큰 컴퓨터 클러스터에서 동작하는 분산 응용 프로그램을 지원하는 프레웨어 자바 소프트웨어 프레임워크**

- RDBMS와 맵리듀스 비교

||전통적인 RDBMS|Hadoop|
|------|---|---|
|데이터 크기|기가바이트|페타바이트|
|데이터 형식|In this structured data is mostly processed|In this both structured and unstructured data is processed|
|접근 방식|대화형과 일괄 처리 방식|일괄 처리 방식|
|변경|여러 번 읽고 쓰기|한 번 쓰고 여러 번 읽기|
|구조|쓰기 기준 스키마 (Schema on Write)|읽기 기준 스키마 (Schema on Read)|
|무결성|높음|비교적 낮음|
|확장성|비교적 낮음|높음|

<details>
<summary>Schema on Read / Schema on Write</summary>
<div markdown="1">
<br>       
   
Schema on read는 문자 그대로, 데이터를 읽을 때 스키마가 정의되어 읽는다는 것이다. 이와 다르게 schema on write는 데이터를 처음 저장할때 스키마를 정의하고 데이터를 저장하는 것이다.
   
빅데이터 관점에서의 Schema on read의 장점은 데이터를 원본 그대로 저장할 수 있다는 것이다.  
만약에 schema on write라면, 데이터를 저장하는 과정에서부터 데이터가 변형되어야 하며 이 변형 자체가 원본 데이터의 부분적인 유실이 될 수 있기 때문이다.   
데이터를 잘 알고, 자주 사용한다고 했을 경우에는 schema on write가 적합할 수도 있다. 그러나 빅데이터에서는 정형, 비정형 데이터가 실시간으로 굉장히 많이 들어오기 때문에 스키마를 정의하기 어려운 경우도 있으므 Schema on read를 사용하는게 더 좋을 수도 있다.

</div>
</details>

RDBMS는 수 밀리초 ~ 수 초 정도의 짧은 응답시간을 요구하고, Hadoop에서는 분산처리를 위한 전처리가 필요하여 최저 10 ~ 20초 정도의 오버헤드가 발생한다.  
하둡은 비정형 분석과 같이 일괄 처리 방식으로 전체 데이터셋을 분석할 필요가 있는 문제에 적합하다. RDBMS는 상대적으로 작은 양의 데이터를 낮은 지연 시간에 추출하고 변경하기 위해 데이터셋을 색인하기 때문에 특정 쿼리와 데이터 변경에 적합하다. 하둡은 데이터를 한 번 쓰고 여러 번 읽는 애플리케이션에 적합하지만, 관계형 데이터베이스는 지속적으로 변경되는 데이터셋에 적합하다고 할 수 있다.  

- hadoop cluster  
클러스터란, 특정한 기능수행을 위해 여러 대의 컴퓨터가 네트워크로 연결된 것을 의미하며, 이때 클러스터를 구성하는 개별 컴퓨터를 node라고 한다.
여러 개의 노드가 모여 하나의 rack을 구성하고 rack이 모여 하나의 hadoop cluster를 구축하게 된다.
rack은 물리적으로 같은 network의 switch에 모두 연결 되어 있다. 그렇기 때문에 두 노드의 대역폭은 다른 rack에 있는 노드보다 크게 된다. 즉 데이터의 이동을 할 수 있는 폭이 크기 때문에 데이터 속도가 빠르다. network의 다른 switch에 연결되어 있는 rack으로 인해 성능저하가 발생할 수 있다.
<img width="472" alt="hadoop cluster" src="https://user-images.githubusercontent.com/55703132/110307981-e26e2c00-8042-11eb-9134-5f345a3caa68.png">

<br>

## 💻 공부할 내용

- ### [MapReduce](#1-맵리듀스)
- ### [HDFS](#2-hdfs-hadoop-distributed-filesystem)
- ### [YARN](#3-yarn)
- ### [하둡 운영 및 관리](#4-하둡-운영-및-관리)

<br>

---

## 1. 맵리듀스
대용량의 데이터 처리를 위한 분산 병렬 프로그래밍 모델, 소프트웨어 프레임워크  
맵 리듀스 프레임워크를 이용하면 대규모 분산 컴퓨팅 환경에서 대량의 데이터를 병렬로 분석 가능  
프로그래머가 직접 작성하는 맵과 리듀스라는 두개의 메서드로 구성  

- Map  
흩어져 있는 데이터를 연관성 있는 데이터들로 분류하는 작업. (key, value 형태)
- Reduce  
Map에서 출력된 데이터에서 중복 데이터를 제거하고 원하는 데이터를 추출하는 작업

하둡은 데이터의 일부분이 저장된 클러스터의 각 머신에서 맵리듀스 프로그램을 실행한다. 이를 위해 하둡은 YARN(하둡 자원 관리 시스템)을 이용한다.

### 데이터 흐름
**잡Job**은 클라이언트가 수행하는 작업의 기본 단위이다. 하둡은 잡을 **map task**와 **reduce task**로 나누어 실행한다. 각 태스크는 YARN을 이용하여 스케줄링되고 클러스터의 여러 노드에서 실행된다. 특정 노드의 태스크가 하나 실패하면 자동으로 다른 노드를 재할당 하여 다시 실행된다.  

하둡은 맵리듀스 잡의 입력을 input split(또는 단순히 split)이라고 부르는 고정 크기 조각으로 분리한다. 하둡은 각 스플릿마다 하나의 맵 태스를 생성하고 스플릿의 각 레코드를 사용자 정의 맵 함수로 처리한다. 일반적인 잡의 적절한 스플릿 크기는 HDFS 블록의 기본 크기인 128mb가 적당하다고 알려져 있다.  

하둡은 HDFS 내의 입력 데이터가 있는 노드에서 맵 태스크를 실행할 때 가장 빠르게 작동한다(a). 이를 **data locality optimization(데이터 지역성 최적화)** 라고 하는데, 클러스터의 중요한 공유자원인 네트워크 대역폭을 사용하지 않는 방법이다. 그러나 맵 태스크의 입력 스플릿에 해당하는 HDFS 블록 복제본이 저장된 세 개의 노드 모두 다른 맵 태스크를 실행하여 여유가 없는 상황(데이터 지역성을 위한 가용 슬롯이 없는)이 발생할 수도 있다. 이런 상황에서 잡 스케줄러는 블록 복제본이 저장된 동일 랙에 속한 다른 노드에서 가용한 맵 슬롯을 찾는다(b). 또한 아주 드문 일이지만 데이터 복제본이 저장된 노드가 없는 외부 랙의 노드가 선택될 수도 있는데, 이때에는 랙 간 네트워크 전송이 불가피하게 일어난다(c).

<img width="350" alt="맵 태스크" src="https://user-images.githubusercontent.com/55703132/110323875-90d09c00-8058-11eb-8c5f-2a25bdaa6fad.png" />

**맵 태스크 결과는 HDFS가 아닌 로컬 디스크에 저장된다. 리듀스의 결과는 HDFS애 저장된다.**   
리듀스 태스크로 모든 결과를 보내기 전에 맵 태스크가 실패한다면 하둡은 자동으로 해당 맵 태스크를 다른 노드에 할당하여 맵의 출력을 다시 생성할 것이다. 리듀스 태스크는 일반적으로 모든 매퍼의 출력 결과를 입력으로 받기 때문에 데이터 지역성의 장점이 없다. 정렬된 맵의 모든 결과는 네트워크를 통해 일단 리듀스 태스크가 실행 중인 노드로 전송되고, 맵의 모든 결과를 병합 후 사용자 정의 리듀스 함수로 전달된다.  
리듀스 출력에 대한 HDFS 블록의 첫번째 복제본은 로컬 노드에 저장되고, 나머지 복제본은 외부 랙에 저장된다.

### Combiner function
클러스터에서 맵리듀스 잡이 사용하는 네트워크 대역폭은 한계가 있기 때문에 맵과 리듀스 태스크 사이의 데이터 전송을 최소화할 필요가 있다. 하둡은 맵의 결과를 처리하는 **combiner function**을 허용한다 (컴바이너 함수의 출력이 결국 리듀스 함수의 입력이 된다). 컴바이너 함수는 최적화와 관련이 있다. 하둡은 컴바이너 함수의 호출 빈도와 상관없이 리듀스의 결과가 언제나 같도록 보장한다.

ex. 해당 연도에 최고기온 구하기   
<img width="600" src="https://user-images.githubusercontent.com/55703132/111160946-f2a37f80-85dd-11eb-9b23-6b179c375ebe.JPG" />

컴바이너 추가   
<img width="650" src="https://user-images.githubusercontent.com/55703132/111161013-018a3200-85de-11eb-9800-02707d21ef22.JPG" />


<br>

❗ 아래 맵리듀스에 관한 내용은 [HDFS](#2-hdfs-hadoop-distributed-filesystem)와 [YARN](#3-yarn)을 보고난 후 다시 보자 ❗

### 맵리듀스 잡 실행 상세분석 ⭐
[[여기]](#yarn-application-수행) 같이 참고  
- **클라이언트** : 맵리듀스 잡을 제출
- **YARN Resource Manager** : 클러스터 상에 계산 리소스의 할당을 제어
- **YARN Node Manager** : 클러스터의 각 머신에서 계산 컨테이너를 시작하고 모니터링
-  **MapReduce Application Master** : 맵리듀스 잡을 수행하는 각 Task를 제어. AM와 MapReduce task는 컨테이너 내에서 실행되며, RM는 잡을 할당하고 NM는 태스크를 관리하는 역할을 맡는다.
-  **Distributed FileSystem** : 다른 단계 간에 잡 리소스 파일들을 공유하는 데 사용된다(보통 HDFS 사용)


하둡이 맵리듀스 잡을 실행하는 방식  
<img width="700" alt="하둡이 맵리듀스 잡을 실행하는 방식" src="https://user-images.githubusercontent.com/55703132/111122418-611e1880-85b1-11eb-8e3f-dcb7e8c34185.JPG" />

1. 클라이언트에서 맵리듀스 잡을 실행한다.

2. Resource Manager에 맵리듀스 잡ID로 사용될 새로운 애플리케이션 ID를 요청한다.   
잡의 출력 명세를 확인한다. 예를 들어 출력 디렉터리가 지정되지 않았거나 이미 존재한다면 해당 잡은 제출되지 않고 맵리듀스 프로그램에 에러를 전달한다.   
잡의 입력 스플릿(input split)을 계산한다. 스플릿을 계산할 수 없다면(예를 들어 입력 경로가 없다면) 잡은 제출되지 않고 맵리듀스 프로그램에 에러를 전달한다.

3. 잡 실행에 필요한 잡 JAR 파일, 환경 설정 파일, 계산된 입력 스플릿 등의 잡 리소스를 공유 파일시스템에 있는 해당 잡 ID 이름의 디렉터리에 복사한다.

4. RM를 호출하여 잡을 제출한다.

5. RM가 YARN 스케줄러에 요청을 전달한다. 스케줄러는 컨테이너를 하나 할당하고, RM는 NM의 운영 규칙에 따라 AM process를 시작한다.

6. Job 초기화.  
AM는 잡을 초기화할 때 잡의 진행 상태를 추적하기 위한 다수의 bookkeeping(장부) 객체를 생성한다. 이후에 각 태스크로부터 진행 및 종료 보고서를 받는다.

7. 클라이언트가 계산한 입력 스플릿 정보를 공유 파일시스템에서 읽어온다.  
입력 스플릿 별로 맵 태스크 객체를 생성한다.  

AM는 맵리듀스 잡을 구성하는 태스크를 실행할 방법을 결정해야 한다. (잡의 크기가 작다면 AM는 태스크를 자신의 JVM에서 실행할 수도 있다 => 잡이 우버되었다. 우버 태스크로 실행된다.)   

8. 잡을 우버 태스크로 실행하기 적합하지 않다면 AM는 RM에 잡의 모든 맵과 리듀스 태스크를 위해 컨테이너를 요청한다. RM의 스케줄러가 특정 노드 상의 컨테이너를 위한 리소스를 태스크에 할당하면 

9. AM는 NM와 통신하며 컨테이너를 시작한다.

10. 태스크를 실행하기 전에 잡 환경 설정, JAR파일, 분산 캐시와 관련된 파일 등 필요한 리소스를 로컬로 가져와야 한다.

11. 최종적으로 맵과 리듀스 태스크를 실제 실행한다.  
(리듀스 태스크는 클러스터 어드 곳에서도 실행될 수 있지만, 맵 태스크 요청은 스케줄러가 최대한 준수하는 [data locality](#데이터-흐름) 제약이 있다.)

### 실패
1. Task 실패  
AM는 태스크 시도 실패를 알게 되면 해당 태스크 실행을 다시 스케줄링한다.

2. Application Master 실패   
복구 작업 방식은 다음과 같다.  
AM는 주기적으로 RM에 heartbeat를 보내고, AM의 실패 이벤트 발생  시 RM는 이 실패를 감지하고 새로운 컨테이너(NM가 운영하는)에서 실행한 새로운 마스터 인스턴스를 시작한다.

3. Node Manager 실패  
NM가 crash에 의해 실패하거나 굉장히 느리게 수행 중이라면 RM에 heartbeat 전송을 중단할 것이다(혹은 굉장히 드물게 전송한다). RM는 heartbeat 전송을 중단한 NM가 일정시간동안 한번도 전송하지 않음을 인지하면 이를 컨테이너를 스케줄링하는 노드 풀에서 제거한다. 실패한 NM에서 수행 중인 AM나 task는 앞서 기술한 메커니즘을 통해 복구 될 것이다.  
애플리케이션 실패 횟수가 높으면 NM 자체가 실패하지 않았더라도 NM는 블랙리스트에 등록된다. 블랙리스트 등록은 AM가 한다.

4. Resource Manager 실패  
심각한 상황. RM 없이는 잡이나 태스크 컨테이너가 실행될 수 없기 때문. RM는 single point of failure ([네임노드](네임노드와-데이터노드)도 SPOF입니당~)   
high availability을 달성하기 위해서는 두 개의 RM를 active-standby 설정으로 실행해야 한다. 실행 중인 애플리케이션에 대한 모든 정보를 고가용 상태 저장소(주키퍼 혹은 HDFS)에 보관되기 때문에 standby RM는 실패한 active RM의 핵심 상태를 복구할 수 있다. 그리고 NM의 정보는 상태 저장소에 보관되지 않는데, 이는 NM가 첫 번째 heartbeat를 전송할 때 새로운 RM가 상대적으로 빠르게 재구축할 수 있기 때문이다 (**태스크는** RM 상태의 일부가 아니며 **AM가 관리함을 명심!** 따라서 보관할 상태 정보는 MapRedcue 1의 job tracker에 비해 굉장히 적다).


### 셔플과 정렬
**Shuffle** : 맵리듀스는 모든 리듀서의 입력이 키를 기준으로 정렬되는 것을 확실히 보장한다. **시스템이 이러한 정렬을 수행하고 맵의 출력을 리듀서의 입력으로 전송하는 과정**이 셔플이다.

<img width="700" alt="맵리듀스에서 셔플과 정렬" src="https://user-images.githubusercontent.com/55703132/111146542-e8c55080-85cc-11eb-8252-e77ca925d00c.JPG" />

- Map 부분   
각 map task는 메모리 버퍼를 가지고 있으며 이곳에 결과를 기록한다. 버퍼의 내용이 특정 한계치에 도달하면 백그라운드 스레드가 디스크에 **spill**(각 파티션별 메모리 버퍼를 디스크에 한번에 쓰는 것)하기 시작한다.  
디스크에 쓰기 전에 스레드는 먼저 데이터를 최종적으로 전송할 리듀서 수에 맞게 파티션으로 나눈다.

- Reduce 부분   
   - 복사 단계     
   리듀스 태스크는 클러스터 내에 퍼져 있는 많은 맵 태스크로부터 특정 파티션에 해당하는 맵 출력을 필요로 한다. 맵 태스크는 각기 다른 시간에 끝날 수 있으므로 리듀스 태스크는 각 맵 태스크의 출력이 끝나는 즉시 복사하기 시작한다.
   
   - 정렬 단계 (혹은 병합 단계)   
   모든 맵 출력이 복사되는 시점에 리듀스 태스크는 정렬 단계로 이동하여 맵 출력을 병합하고 정렬 순서를 유지한다.  
   
   - 리듀스 단계   

<details>
<summary>리듀서는 맵 출력을 인출할 서버를 어떻게 알까?</summary>
<div markdown="1">

**맵 태스크가 성공적으로 종료되면 하트비트 전송 메커니즘을 통해 Application Master에 알려준다.** 그러므로 특정 잡에 대해 AM는 맵 출력과 호스트 사이의 매핑 정보를 알고 있다. 그리고 **리듀서 내의 한 스레드는 맵 출력 호스트 정보를 주기적으로 마스터에 요청**하며 이는 모든 정보를 얻을 때까지 실행된다.

</div>
</details>
<br>

## 2. HDFS (Hadoop Distributed FileSystem)
분산 파일 시스템이란?  
네트워크로 연결된 여러 머신의 스토리지를 관리하는 파일시스템

하둡은 HDFS라는 분산 파일시스템을 제공한다.

### 블록  
> HDFS 블록이 큰 이유는?  
> HDFS 블록(128 MB)은 디스크 블록(512 Btye)에 비해 상당히 크다. 그 이유는 **탐색 비용을 최소화**하기 위해서다. 블록이 매우 크면 블록의 시작점을 탐색하는 데 걸리는 시간을 줄일 수 있고 데이터를 전송하는 데 더 많은 시간을 할애할 수 있다.

분산 파일시스템에 **블록 추상화**의 개념이 도입하면서 얻게 된 이득이 있다.
1. 파일 하나의 크기가 단일 디스크의 용량보다 더 커질 수 있다.  
하나의 파일을 구성하는 여러 개의 블록이 동일한 디스크에만 저장될 필요가 없으므로 클러스터에 있는 어떤 디스크에도 저장될 수 있다. (-> 파일은 블록으로 나뉠 수 있고, 블록은 디스크에 저장된다)

2. 파일 단위보다는 블록단위로 추상화를 하면 스토리지의 서브시스템을 단순하게 만들 수 있다.  
블록은 고정 크기고 저장에 필요한 디스크 용량만 계산하면 되기 때문에 메타데이터에 대한 고민을 덜 수 있다. 블록은 단지 저장된 데이터의 청크일 뿐이고 권한 정보와 같은 파일의 메타데이터는 블록과 함께 저장될 필요가 없으므로 별도의 시스템에서 다루도록 분리할 수 있다.

3. 블록은 내고장성(fault tolerance)과 가용성(availability)을 제공하는 데 필요한 복제(replication)를 구현할 때 매우 적합하다.  
블록의 손상과 디스크 및 머신의 장애에 대처하기 위해 각 블록은 물리적으로 분리된 다수의 머신(보통 3개)에 복제된다.

### 네임노드와 데이터노드
HDFS 클러스터는 master-worker 패턴으로 동작하는 두 종류의 노드(**마스터인 하나의 네임노드와 워커인 여러 개의 데이터노드**)로 구성되어 있다.  
**네임노드는 파일 시스템의 네임스페이스를 관리**한다. 네임노드는 파일시스템 트리와 그 트리에 포함된 모든 파일과 디렉터리에 대한 메타데이터를 유지한다. 이 정보는 namespace image와 edit log라는 두 종류의 파일로 로컬 디스크에 영속적으로 저장된다. 또한 **파일에 속한 모든 블록이 어느 데이터노드에 있는지 파악하고 있다.** 하지만 블록의 위치 정보는 시스템이 시작할 때 모든 데이터노드로부터 받아서 재구성하기 때문에 디스크에 영속적으로 저장하지는 않는다.  

HDFS 클라이언트는 사용자를 대신해서 네임노드와 데이터노드 사이에서 통신하고 파일시스템에 접근.  

**데이터노드는 클라이언트나 네임노드의 요청이 있을 때 블록을 저장하고 탐색하며, 저장하고 있는 블록의 목록을 주기적으로 네임노드에 보고한다.** 데이터 노드는 디스크에 저장된 블록을 읽는다.

네임노드가 없으면 파일시스템은 동작하지 않는다. 따라서 네임노드의 장애복구 기능은 필수적!  
하둡은 두가지 메커니즘 제공  
1. 파일로 백업  
네임노드가 다수의 파일시스템에 영구적인 상태를 저장하도록. 주로 권장하는 방법은 로컬 디스크와 원격의 NFS(Network FileSystem) 마운트 두 곳에 동시에 백업하는 것이다.  
2. [보조 네임노드(secondary namenode) 운영](#fsimage-and-edits-log)   
secondary namenode의 주 역할은 edit log가 너무 커지지 않도록 주기적으로 namespace image를 edit log와 병합하여 새로운 namespace image를 만드는 것이다. 또한 secondary namenode는 주 네임노드에 장애가 발생할 것을 대비해서 namespace image 복제본을 보관하는 역할도 맡는다.  

### HDFS federation
네임노드는 파일시스템의 모든 파일과 각 블록에 대한 참조 정보를 메모리에서 관리한다. 따라서 파일이 매우 많은 대형 클러스에서 **확장성**에 가장 큰 걸림돌이 되는 것은 바로 **메모리**다. **네임노드의 확장성 문제를 해결하기 위해** 하둡은 **HDFS federation**(연합체) 을 지원하고 있다. HDFS federation을 활용하면 각각의 네임노드가 파일시스템의 네임스페이스(소속을 나타낸다?) 일부를 나누어 관리하는 방식으로 새로운 네임노드를 추가할 수 있다. (ex. namenode A는 /user 디렉터리 아래 모든 파일관리. namenode B는 /share 디렉터리 아래 모든 파일관리)  

HDFS federation을 적용하면 각 네임노드는 네임스페이스의 메타데이터를 구성하는 **namespace volume**과 네임스페이스에 포함된 파일의 전체 블록을 보관하는 **block pool**을 관리한다. namespace volume은 서로 독립되어 있으며, 따라서 네임노드는 서로 통신할 필요가 없고, 특정 네임노드에 장애가 발생해도 다른 네임노드가 관리하는 네임스페이스의 가용성에 영향을 주지 않는다. 하지만 block pool의 저장소는 분리되어 있지 않다! 모든 데이터노드는 클러스터의 각 네임노드마다 등록되어 있고, 여러 block pool로부터 블록을 저장한다.

### HDFS 고가용성(HA: high availability)
※ HA: 서버와 네트워크, 프로그램 등의 정보 시스템이 상당히 오랜 기간 동안 지속적으로 정상 운영이 가능한 성질  
※ 네임노드는 single point of failure(SPOF): 네임노드에 장애가 발생하면 맵리듀스 잡을 포함하여 모든 클라이언트가 파일을 읽거나 쓰거나 조회할 수 없게 된다.

High avaliability은 active-standby 상태로 설정된 한 쌍의 네임노드로 구현된다. active namenode에 장애가 발생하면 standby namenode가 그 역할을 이어받아 큰 중단 없이 클라이언트의 요청을 처리한다.  
- 네임노드는 edit log를 공유하기 위해 HA shared storage를 반드시 사용해야 한다. standby namenode가 활성화되면 먼저 기존 active namenode의 상태를 동기화하기 위해 공유 edit log를 읽고, 이어서 active namenode에 새로 추가된 항목도 마저 읽는다.
- 데이터노드는 block report를 두 개의 네임노드에 보내야 한다. 블랙 매핑 정보는 디스크가 아닌 네임노드의 메모리에 보관되기 때문.
- HA에서 standby namenode는 secondary namenode의 역할을 포함하고 있으며, active namenode namespace의 체크포인트 작업을 주기적으로 수행한다.

### 데이터 흐름
- HDFS로부터 데이터 **읽기**  

<img width="500" alt="hdfs-data-flow-read" src="https://user-images.githubusercontent.com/55703132/111037294-c4e7fa80-8466-11eb-8ef6-7cafe8d05bd7.JPG">

1. 클라이언트가 원하는 파일을 연다.  

2. DistributedFileSystem은 파일의 첫 번째 블록 위치를 파악하기 위해 RPC을 사용하여 네임노드를 호출한다.  
네임노드는 블록별로 해당 블록의 복제본을 가진 데이터노드의 주소를 반환. 이때 클러스터의 네트워크 위상에 따라 클라이언트와 가까운 순으로 데이터노드가 정렬 (대역폭: 동일 노드 > 동일 랙의 다른 노드 > 동일 데이터 센터에 있는 다른 랙의 노드 > 다른 데이터 센터에 있는 노드)
<details>
<summary>RPC (remote procedure call)</summary>
<div markdown="1">

RPC(Remote Procedure call)란, 별도의 원격 제어를 위한 코딩 없이 다른 주소 공간에서 리모트의 함수나 프로시저를 실행 할 수 있게 해주는 프로세스간 통신.  
즉, 위치에 상관없이 RPC를 통해 개발자는 원하는 함수를 사용할 수 있다.

운영체제를 공부하다 보며 프로세스간 통신을 위해 IPC(inter-Process Communication)을 이용하는 내용을 볼 수 있는데, RPC는 IPC 방법의 한 종류로 원격지의 프로세스에 접근하여 프로시저 또는 함수를 호출하여 사용하는 방법을 말한다.

</div>
</details>

3. DFS가 FSDataInputStream을 반환. 클라이언트는 스트림을 읽기 위해 read() 메서드 호출한다.

4. DFSInputStream은 가장 가까운(첫번째) 데이터노드와 연결한다. 해당 스트림에 대해 read() 메서드를 반복적으로 호출하면 데이터노드에서 클라이언트로 모든 데이터가 전송된다.

5. 블록의 끝에 도달하면 DFSInputStream은 데이터노드의 연결을 닫고 다음 블록의 데이터노드를 찾는다.
클라이언트는 스트림을 통해 블록을 순서대로 하나씩 읽는다. 클라이언트는 다음 블록의 데이터노드 위치를 얻기 위해 네임노드를 호출한다.

6. 모든 블록에 대한 읽기가 끝나면 클라이언트는 close() 메서드를 호출한다.

이러한 설계의 핵심은 **클라이언트는 데이터를 얻기 위해 데이터노드에 직접적으로 접촉하고, 네임노드는 각 블록에 적합한 데이터노드를 안내해준다는 것!**
데이터 트래픽은 클러스터에 있는 모든 데이터노드에 고르게 분산되므로 HDFS는 동시에 실행되는 클라이언트의 수를 크게 늘릴 수 있다.

<br>

- 데이터를 HDFS에 **쓰기**

<img width="500" alt="hdfs-data-flow-write" src="https://user-images.githubusercontent.com/55703132/111037865-58222f80-8469-11eb-8c89-6476f1f78504.JPG">

1. create()를 호출하여 파일을 생성한다.

2. DFS은 파일시스템의 네임스페이스에 새로운 파일을 생성하기 위해 네임노드에 RPC 요청을 보낸다.
네임노드는 요청한 파일과 동일한 파일이 이미 존재하는지, 클라리언트가 파일을 생성할 권한을 가지고 있는지 등 다양한 검사를 수행. 검사 통과하면 네임노드는 새로운 파일의 레코드를 만듦.

3. DFS는 데이터를 쓸 수 있도록 FSDataOutputStream을 반환. 클라이언트가 데이터를 쓴다.
(읽을 때와 마찬가지로 FSDataOutputStream은 데이터노드와 네임노드의 통신을 처리하는 DFSOutputStream으로 래핑된다.)

4.
   1. DFSOutputStream은 데이터를 패킷으로 분리하고, **data queue**라 불리는 내부 큐로 패킷을 보낸다.
   2. 네임노드에 복제본을 저장할 데이터노드의 목록을 요청한다.
   3. 데이터노드 목록에 포함된 노드는 파이프라인을 형성. 복제 수준이 3이면 세 개의 노드가 파이프라인에 속하게 된다.
   4. DataStreamer(데이터 큐에 있는 패킷을 처리)는 파이프라인의 첫 번째 데이터노드로 패킷을 전송 -> 첫 번째 데이터노드는 각 패킷을 저장하고 그것을 파이프라인의 두 번째 데이터노드로 보냄 -> 두번째 데이토노드는 받은 패킷을 저장하고 마지막 데이터노드로 전달

5. DFSOutputStream은 데이터노드의 승인 여부를 기다리는 **ack queue**라 불리는 내부 패킷 큐를 유지한다. ack queue에 있는 패킷은 파이프라인의 모든 데이터노드로부터 ack 응답을 받아야 제거된다.

6. 데이터 쓰기를 완료할 때 클라이언트는 close() 메서드를 호출한다.

7. 데이터노드 파이프라인에 남아 있는 모든 패킷을 flush하고 승인이 나기를 기다린다. 모든 패킷이 완전히 전송되면 네임노드에 'file is complete' 신호를 보낸다.

<br>

복제본 배치

<img width="300" alt="replication_pipeline" src="https://user-images.githubusercontent.com/55703132/111065116-6a4cae00-84fb-11eb-89be-97c06b13f6ee.jpg" />

첫 번째 복제본을 클라이언트와 같은 노드에 배치한다 (만약 클라이언트가 클러스터 외부에 있으면(데이터노드가 아니면) 무작위로 노드 선택). 두 번째 복제본은 첫 번째 노드와 다른 랙의 노드에 배치된다. 세 번째 복제본은 두 번째 노드와 같은 랙의 다른 노드에 배치된다. 그 이상의 복제본은 클러스터에서 무작위로 선택하여 배치한다.  

<br>

## 3. YARN
클러스터 자원 관리 시스템. (Yet Another Resource Negotiator)

<img width="400" alt="yarn_application" src="https://user-images.githubusercontent.com/55703132/111068567-9f153100-850c-11eb-8e93-fdae38bb4f0b.png" />

맵리듀스, 스파크 등과 같은 분산 컴퓨팅 프레임워크는 클러스터 계산 계층(YARN)과 클러스터 저장 계층(HDFS와 HBase) 위에서 **YARN Application**을 실행한다.

### YARN Application 수행
YARN은 두 가지 유형의 장기 실행 데몬을 통해 핵심 서비스를 제공한다.  
**클러스터에서 유일**한 **Resource Manager**는 클러스터 전체 자원의 사용량을 관리.   
**모든 머신에서** 실행되는 **Node Manager**는 **컨테이너**를 구동하고 모니터링하는 역할.  

- YARN이 Application을 구동하는 방식

<img width="500" alt="yarn이 application을 구동하는 방식" src="https://user-images.githubusercontent.com/55703132/111069459-8149cb00-8510-11eb-9fc8-6859d5961b1d.png" />

1. 클라이언트는 YARN에서 애플리케이션을 구동하기 위해 Resource Manager에 접속하여 **Application Master** 프로세스의 구동을 요청한다.

2. RM는 컨테이너에서 Application Master(map-reduce job당 하나)를 시작할 수 있는 Node Manager를 하나 찾는다.   
AM가 딱 한 번만 실행될지는 애플리케이션에 따라 다르다. AM가 단순한 계산을 단일 컨테이너에서 수행하고 그 결과를 클라이언트에 반환한 후 종료되거나, 
<details>
<summary>Application Master</summary>
<div markdown="1">

하나의 프로그램에 대한 마스터 역할을 수행하며, 스케줄러로부터 적절한 컨테이너를 할당 받고, 프로그램 실행 상태를 모니터링하고 관리한다.

</div>
</details>

3. RM에 더 많은 컨테이너를 요청한 후

4. 분산 처리를 수행하는 경우도 있다.

<br>

- 자원 요청
YARN 애플리케이션은 실행 중에는 아무 때나 자원 요청을 할 수 있다.
   - 처음에 모든 요청   
   스파크는 클러스터에서 고생 개수의 executor를 시작한다.
   - 유동적인 접근이 필요한 경우, 애플리케이션의 요구에 따라 동적으로 자원을 추가로 요청   
   맵리듀스는 두단계. 처음에 필요한 맵 태스크 컨터에너를 요청한다. 맵 태스크가 어느 정도 실행된 후에 리듀스 태스크 컨테이너가 시작될 수 있다.
   
### YARN과 맵리듀스1의 차이점
※ 하둡1과 그 이전버전의 맵리듀스 분산 구현은 '맵리듀스 1'로, YARN(하둡 2와 이후 버전)을 이용한 구현은 '맵리듀스 2'로 구분해서 언급한다.

MapReduce 1과 YARN 컴포넌트 비교  
|MapReduce 1|YARN|
|------|---|
|Job tracker|Resource Manager, Application Master, Timeline server|
|Task tracker|Node Manager|
|Slot|Container|

- Job tracker   
여러 Task tracker에서 실행되는 태스크를 스케줄링함으로써 시스템에서 실행되는 모든 Job을 조율한다.   
Job scheduling : Task와 Task tracker를 연결.   
태스크 진행 모니터링 : 태스크를 추적하고, 실패하거나 느린 태스크를 다시 시작하고, 전체 카운터를 유지하는 방법으로 태스크 장부(bookkeeping)를 기록한다.  
YARN은 이러한 역할을 RM와 AM를 통해 처리한다.

- Task tracker   
태스크를 실행하고 진행 상황을 job tracker에 전송하기 때문에 job tracker는 각 jbo의 전체적인 진행 상황을 파악할 수 있다.

<br>

YARN을 사용하여 얻을 수 있는 이익
- 확장성   
YARN은 맵리듀스1보다 큰 클러스터에서 실행될 수 있다. 맵리듀스 1에서는 job tracker가 job과 task를 모두 관리하기 때문에 4,000 노드나 40,000 태스크를 넘어서면 병목현상이 발생한다. 하지만 YARN은 RM과 AM를 분리하는 구조이므로, 더 많은 노드와 태스크까지 확장할 수 있도록 설계되었다 (10,000 노드, 100,000 태스크).

- 가용성   
job tracker의 메모리에 있는 복잡한 상태 정보가 매우 빠르게 변경되는 상황에서 job tracker 서비스에 HA를 적용하는 것은 매우 어려운 일이다. 각 태스크의 상태는 수 초마다 변경되기 때문이다.  
job tracker의 역할이 YARN에서는 RM와 AM로 분리되었기 때문에 HA 적용 가능하다. RM와 맵리듀스 잡을 위한 AM 모두에 HA를 제공한다.

- 효율성   
MapReduce 1에서 각 task tracker는 map slot과 reduce slot으로 구분된 고정 크기 'slot'의 정적 할당 설정을 가지고 있다. map slot은 map task 실행에만, reduce slot은 reduce task 에만 사용할 수 있다.  
YARN에서 Node Manager는 정해진 개수의 슬록 대신 일종의 리소스 풀을 관리한다. YARN의 자원은 잘게 쪼개져 있기 때문에 애플리케이션은 필요한 만큼의 자원을 요청할 수 있다. 기존에는 개별 슬롯을 사용했기 때문에 특정 태스크를 위해 너무 많거나(자원 낭비) 너무 적게(실패의 원인) 자원을 할당했다.

### YARN scheduling
- FIFO   
대형 잡이 완료될 때까지 작은 잡은 계속 대기해야 한다는 단점이 존재한다.
- Capacity   
작은 잡이 제출되는 즉시 분리된 전용 큐에서 처리해준다. 해당 큐는 잡을 위한 자원을 미리 예약해두기 때문에 전체 클러스터의 효율성은 떨어진다. 또한 대형 잡은 FIFO 스케줄러보다 늦게 끝나게 된다.  
예를 들면, 회사에서 각 조직이 전체 클러스터의 지정된 가용량을 미리 할당받는 것이다.
- Fair  
실행 중인 모든 잡의 자원을 동적으로 분배하기 때문에 미리 자원의 가용량을 예약할 필요가 없다.  
실행 중인 모든 애플리케이션에 동일하게 자원을 할당한다. 하지만 균등한 공유는 **큐 사이**에만 실제로 적용된다.

<img width="270" alt="FIFO_scheduler" src="https://user-images.githubusercontent.com/55703132/111071180-43e93b80-8518-11eb-85d8-e58e0cfa2eb2.JPG" /> <img width="270" alt="Capacity_scheduler" src="https://user-images.githubusercontent.com/55703132/111071183-477cc280-8518-11eb-8296-c00b37fd04f4.JPG" /> <img width="270" alt="Fair_scheduler" src="https://user-images.githubusercontent.com/55703132/111071185-49df1c80-8518-11eb-8fb9-5ec986ced4df.JPG" />  

Fair Scheduler. 사용자 사이에만 균등하게 공유 된다고 볼 수 있다.

<img width="350" alt="사용자 큐 간의 균등 공유" src="https://user-images.githubusercontent.com/55703132/112646641-0dea7680-8e8b-11eb-9f99-6d009cb75d07.jpg" />

<br>

## 4. 하둡 운영 및 관리
### 보안 - Kerberos와 Hadoop  
Kerberos : **티켓(ticket)** 을 기반으로 동작하는 컴퓨터 네트워크 인증 암호화 프로토콜   
비보안 네트워크에서 통신하는 노드가 보안 방식으로 다른 노드에 대해 식별할 수 있게 허용한다.  

Kerberos를 사용할 때 클라이언트가 이 서비스를 이용하려면 각 단계에서 서버와의 메시지 교환을 수반하는 다음 세 단계를 거쳐야 한다.  
1. **인증.**    
클라이언트는 인증 서버에 자신을 인증한다. 그리고 시간 정보가 포함된 티켓-승인 티켓 Ticket-Granting Ticket(TGT)을 수신한다.

2. **권한 부여.**   
클라이언트는 TGT를 이용하여 티켓 승인 서버에 서비스 티켓을 요청한다.

3. **서비스 요청.**    
클라이언트는 서비스 티켓을 이용하여 클라이언트가 사용할 서비스를 제공하는 서버에 자신을 인증한다. 하둡의 경우 이 서버는 Name node나 Resource Manager가 될 것이다.

<img width="600" alt="kerberos 티켓 교환 프로토콜의 3단계 절차" src="https://user-images.githubusercontent.com/55703132/111272080-7492b780-8675-11eb-9c24-e5550d2ae147.jpg" />

### [FSImage and Edits log](#네임노드와-데이터노드)
파일시스템의 클라이언트가 **쓰기** 동작을 하면 일단 edits log에 해당내역이 기록된다. 네임노드는 파일시스템의 메타데이터를 in-memory(파일과 메모리 양쪽에 데이터를 유지하는 방식)로 관리하는데, edits log를 먼저 변경한 후 메모리상의 메타데이터도 변경한다. 클라이언트의 **읽기** 요청에는 in-memory 메타데이터만 사용된다.   

네임노드는 쓰기 동작이 끝날 때마다 성공했다는 결과를 클라이언트에 알려주기 전에, 변경 내역에 대해 edits log를 flush하여 동기화시킨다.   
파일시스템에서 쓰기 동작이 있을 때마다 fsimage 파일을 변경하지 않는데, fsimage 파일이 기가바이트 크기로 커지면 성능이 매우 느려지기 때문이다. 만약 네임노드에 장애가 발생하면 먼저 fsimage를 메모리에 로드하고 edits log 파일에서 특정 지점 이후에 발생한 변경 내역들을 메모리에 반영하여 파일시스템의 메타데이터를 최신 상태로 복원할 수 있다.

edits log 파일은 무한정 커질 수 있고 네임노드가 구동 중일 때는 특별한 영향을 주지 않지만, 네임노드가 재시작될 경우 매우 큰 edits log의 변경 내역을 모두 적용하기 위해서는 상당한 시간이 걸린다. 이러한 문제의 해결책은 secondary namenode를 운영하는 것이다. **Secondary Namenode의 용도는 Primary Namenode의 메모리에 있는 파일시스템 메타데이터의 체크포인트를 만드는 것이다.** 체크포인팅 작업의 절차는 다음과 같다.

<img width="500" alt="checkpointing_procedure" src="https://user-images.githubusercontent.com/55703132/111279106-ae67bc00-867d-11eb-813e-09bb02b370ec.JPG" />

1. secondary namenode(보조 네임노드)는 primary namenode(주 네임노드)에 사용 중인 edits 파일을 순환할 것을 요청한다. 이제부터 새로 발생하는 edits log는 새로운 파일에 저장된다.
2. secondary namenode는 HTTP GET 방식으로 primary namenode에 있는 최신 fsimage와 edits 파일을 가져온다.
3. secondary namenode는 fsimage 파일을 메모리에 올리고 edits 파일의 각 변경 내역을 적용한다. 그리고 병합된 새로운 fsimage 파일을 생성한다.
4. secondary namenode는 새로운 fsimage 파일을 HTTP PUT 방식으로 primary namenode에 전송하고, primary namenode는 받은 파일을 .ckpt라는 확장자를 가진 임시 파일로 저장한다.
5. primary namenode는 임시 저장한 fsimage 파일의 이름을 변경하여 사용 가능하게 만든다.
